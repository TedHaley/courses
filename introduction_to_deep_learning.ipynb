{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introduction_to_deep_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNga0GnCSq0PrFoX+Nl3Jlc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TedHaley/courses/blob/master/introduction_to_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6ZF8_k3wSG1",
        "colab_type": "text"
      },
      "source": [
        "Introduction to Neural Networks\n",
        "\n",
        "Overview:  \n",
        "Models such as linear regression can be used to make predictions. It makes predictions by generating weights for some parameters and then adding those parameters together, ex:\n",
        "\n",
        "\n",
        "y = m1X1 + m2X2 + b. \n",
        "\n",
        "salary = 10,000 * years_exp + 50,000\n",
        "\n",
        "\n",
        "Where m1 and m2 are the weights. We can also have linear models that have interacting variables, such as:\n",
        "\n",
        "\n",
        "y = (m1X1 * m2X2) + b. \n",
        "\n",
        "salary = (10,000 * years_exp * 1.1* age) + 50,000\n",
        "\n",
        "\n",
        "This is a very basic example of interactivity between variables. Neural networks are similar to this but take it to the next level as they model many iteractions between many inputs, and then use those interactions as inputs for other interactions.\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/280px-Colored_neural_network.svg.png)\n",
        "\n",
        "As nodes increase, so does the ability to capture interactions.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1fdjoM2tBhsQektdjG1882m5TV9FD9rkg)\n",
        "\n",
        "Forward Propogation:  \n",
        "Forward propogation is when we place weights on the lines between nodes. The output node is equal to the sum of the input nodes times the line weights. This operation is dot product.\n",
        "\n",
        "Example:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1eJ5OLqSjIDluwHcomk2tYZbyGHVK_IWi)\n",
        "\n",
        "hidden layer node 0 = (2 * 1) + (3 * 1) = 5\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXNfK1iE6yPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a7241430-eb2b-4fa8-d2d5-5be0986e2adf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "input_data = np.array([2, 3])\n",
        "\n",
        "weights = {\n",
        "    'node_0': np.array([1, 1]),\n",
        "    'node_1': np.array([-1, 1]),\n",
        "    'output': np.array([2, -1]),\n",
        "}\n",
        "\n",
        "# [2, 3] * [1, 1] = (2 * 1) + (3 * 1) = 5 \n",
        "node_0_value = (input_data * weights['node_0']).sum()\n",
        "node_1_value = (input_data * weights['node_1']).sum()\n",
        "output_value = (np.array([node_0_value, node_1_value]) * weights['output']).sum()\n",
        "\n",
        "print('node 0:', node_0_value)\n",
        "print('node 1:', node_1_value)\n",
        "print('Output:', output_value)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "node 0: 5\n",
            "node 1: 1\n",
            "Output: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MguAi-WO9fmE",
        "colab_type": "text"
      },
      "source": [
        "Activation Function:  \n",
        "Activation functions are found within the hidden layers. Activation functions allow hidden layers to capture non-linearities. One popular activation function is ReLU (rectified linear activation), with is 0 below zero, and linear above zero for x. In the below example, we are going to use tanh as our activation function.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1R_aqYRh66slQSt3YY0n1wWRTOtF8DCRa)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxRcbNhP_IYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b3d2d8cc-5224-4926-e76b-2b8cdd64e0ed"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "input_data = np.array([2, 3])\n",
        "\n",
        "weights = {\n",
        "    'node_0': np.array([1, 1]),\n",
        "    'node_1': np.array([-1, 1]),\n",
        "    'output': np.array([2, -1]),\n",
        "}\n",
        "\n",
        "node_0_input = (input_data * weights['node_0']).sum()\n",
        "node_0_output = np.tanh(node_0_input)\n",
        "\n",
        "node_1_input = (input_data * weights['node_1']).sum()\n",
        "node_1_output = np.tanh(node_1_input)\n",
        "\n",
        "output = (np.array([node_1_input, node_1_output]) * weights['output']).sum()\n",
        "\n",
        "print('node 0:', node_0_output)\n",
        "print('node 1:', node_1_output)\n",
        "print('Output:', output)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "node 0: 0.9999092042625951\n",
            "node 1: 0.7615941559557649\n",
            "Output: 1.2384058440442351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GkkQSydRasl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdd04b57-aaab-4211-c22a-06525c5b6b86"
      },
      "source": [
        "# Using ReLU function\n",
        "\n",
        "def relu(input):\n",
        "    '''0 below 0, x above 0.'''\n",
        "    # Calculate the value for the output of the relu function: output\n",
        "    output = max(0, input)\n",
        "    \n",
        "    # Return the value just calculated\n",
        "    return(output)\n",
        "\n",
        "# Calculate node 0 value: node_0_output\n",
        "node_0_input = (input_data * weights['node_0']).sum()\n",
        "node_0_output = relu(node_0_input)\n",
        "\n",
        "# Calculate node 1 value: node_1_output\n",
        "node_1_input = (input_data * weights['node_1']).sum()\n",
        "node_1_output = relu(node_1_input)\n",
        "\n",
        "# Put node values into array: hidden_layer_outputs\n",
        "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
        "\n",
        "# Calculate model output (do not apply relu)\n",
        "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
        "\n",
        "# Print model output\n",
        "print(model_output)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkkiP_6VTWfK",
        "colab_type": "text"
      },
      "source": [
        "Deep Networks  \n",
        " - Deep networks internally build representations of patterns in the data\n",
        " - Partially replace the need for feature engineering\n",
        " - Subsequrnt layers build increasingly sophisticated representations of raw data\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1QMVRnZHgG1FPYHgJ_c5_X1nj3Lws3ffP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voo_GJJ9WWll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fd1dc09-5215-4b44-b881-9e0bf7aa408b"
      },
      "source": [
        "# Multi-layer networks\n",
        "def predict_with_network(input_data, weights):\n",
        "    # Calculate node 0 in the first hidden layer\n",
        "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
        "    node_0_0_output = relu(node_0_0_input)\n",
        "\n",
        "    # Calculate node 1 in the first hidden layer\n",
        "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
        "    node_0_1_output = relu(node_0_1_input)\n",
        "\n",
        "    # Put node values into array: hidden_0_outputs\n",
        "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
        "    \n",
        "    # Calculate node 0 in the second hidden layer\n",
        "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n",
        "    node_1_0_output = relu(node_1_0_input)\n",
        "\n",
        "    # Calculate node 1 in the second hidden layer\n",
        "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n",
        "    node_1_1_output = relu(node_1_1_input)\n",
        "\n",
        "    # Put node values into array: hidden_1_outputs\n",
        "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
        "\n",
        "    # Calculate model output: model_output\n",
        "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
        "    \n",
        "    # Return model_output\n",
        "    return(model_output)\n",
        "\n",
        "weights = {\n",
        "    'node_0_0': np.array([2, 4]),\n",
        "    'node_0_1': np.array([ 4, -5]),\n",
        "    'node_1_0': np.array([-1,  2]),\n",
        "    'node_1_1': np.array([1, 2]),\n",
        "    'output': np.array([2, 7])\n",
        " }\n",
        "input_data = np.array([3, 5])\n",
        "output = predict_with_network(input_data, weights)\n",
        "print(output)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99GtL4umYbdb",
        "colab_type": "text"
      },
      "source": [
        "Training a Network:  \n",
        "Neural networks are trained using labelled data. The value of the weights for each line are changed to get the desired output.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1d65zM3Re55BS62mzqssqJS9XJxlvFaLJ)\n",
        "\n",
        "We use back propogation to go back and re-weight the lines to get the desired output.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=12gM57rbKz0VQ9jYzyyDuNprv0Vy7Bn-e)\n",
        "\n",
        "The challenge becomes when we are trying to make multiple accurate predictions with a static network. Each outcome is associated with its own error.\n",
        "\n",
        "Loss Function  \n",
        "The loss function is used to aggregate errors in many predictions to form a single number. This is a measure of a model's predictive performance.\n",
        "\n",
        "A common loss function is Mean Squared Error (MSE), where we square each error and take the average of the squared errors. We need to optimize the weights of the model to minimize the loss function.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1eKpqtdRmYIyBfiaC9LSpGzRofsbODznL)\n",
        "\n",
        "\n",
        "Optimize the Loss Function  \n",
        "A simple method for optmizing this loss function is by using gradient descent. We take the derivative at the current point to find the slope of the line. We take a step in the opposite direction of the slope to go to a minima.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1V-gP00xaaRqOSzc6TR7YSsVrbMPbAjhN)\n",
        "\n",
        "We need to make sure we don't step too quickly as we might miss the minima. This is why we use learn rate (often 0.01)\n",
        "step size = learning rate * slope\n",
        "\n",
        "Slope Caluclation Example:  \n",
        "Node 1 = 3\n",
        "weight = 2\n",
        "Node 2 = 6\n",
        "Actual target = 10\n",
        "\n",
        "slope = 2 * error * input = 2 * (6-10) * 3 = -24\n",
        "new weight = weight - lr(slope) = 2 - 0.01(-24) = 2.24\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDjCx4j5Ztfj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d54fac69-2fe0-4e40-bc9b-b6dfe247334b"
      },
      "source": [
        "# Coding how weight changes affect accuracy\n",
        "\n",
        "def predict_with_network(input_data, weights):\n",
        "    # Calculate node 0 in the first hidden layer\n",
        "    node_0_input = (input_data * weights['node_0']).sum()\n",
        "    node_0_output = relu(node_0_input)\n",
        "\n",
        "    # Calculate node 1 in the first hidden layer\n",
        "    node_1_input = (input_data * weights['node_1']).sum()\n",
        "    node_1_output = relu(node_1_input)\n",
        "\n",
        "    # Put node values into array: hidden_1_outputs\n",
        "    hidden_1_outputs = np.array([node_0_output, node_1_output])\n",
        "\n",
        "    # Calculate model output: model_output\n",
        "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
        "    \n",
        "    # Return model_output\n",
        "    return(model_output)\n",
        "\n",
        "# The data point you will make a prediction for\n",
        "input_data = np.array([0, 3])\n",
        "\n",
        "# Sample weights\n",
        "weights_0 = {'node_0': [2, 1],\n",
        "             'node_1': [1, 2],\n",
        "             'output': [1, 1]\n",
        "            }\n",
        "\n",
        "# The actual target value, used to calculate the error\n",
        "target_actual = 3\n",
        "\n",
        "# Make prediction using original weights\n",
        "model_output_0 = predict_with_network(input_data, weights_0)\n",
        "\n",
        "# Calculate error: error_0\n",
        "error_0 = model_output_0 - target_actual\n",
        "\n",
        "# Create weights that cause the network to make perfect prediction (3): weights_1\n",
        "weights_1 = {'node_0': [2, 1],\n",
        "             'node_1': [1, 2],\n",
        "             'output': [1, 0]\n",
        "            }\n",
        "\n",
        "# Make prediction using new weights: model_output_1\n",
        "model_output_1 = predict_with_network(input_data, weights_1)\n",
        "\n",
        "# Calculate error: error_1\n",
        "error_1 = model_output_1 - target_actual\n",
        "\n",
        "# Print error_0 and error_1\n",
        "print(error_0)\n",
        "print(error_1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mvh2GThmugh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0fa13ad9-a7a7-4a65-d90b-fba9bd7b274c"
      },
      "source": [
        "# Scaling up to multiple data points\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "weights_0 = {'node_0': [2, 1],\n",
        "             'node_1': [1, 2],\n",
        "             'output': [1, 1]\n",
        "            }\n",
        "\n",
        "weights_1 = {'node_0': [2, 1],\n",
        "             'node_1': [1, 1.5],\n",
        "             'output': [1, 1.5]\n",
        "            }\n",
        "\n",
        "input_data = [np.array([0, 3]), np.array([1, 2]), np.array([-1, -2]), np.array([4, 0])]\n",
        "target_actuals = [1, 3, 5, 7]\n",
        "\n",
        "# Create model_output_0 \n",
        "model_output_0 = []\n",
        "# Create model_output_1\n",
        "model_output_1 = []\n",
        "\n",
        "# Loop over input_data\n",
        "for row in input_data:\n",
        "    # Append prediction to model_output_0\n",
        "    model_output_0.append(predict_with_network(row, weights_0))\n",
        "    \n",
        "    # Append prediction to model_output_1\n",
        "    model_output_1.append(predict_with_network(row, weights_1))\n",
        "\n",
        "# Calculate the mean squared error for model_output_0: mse_0\n",
        "mse_0 = mean_squared_error(model_output_0, target_actuals)\n",
        "\n",
        "# Calculate the mean squared error for model_output_1: mse_1\n",
        "mse_1 = mean_squared_error(model_output_1, target_actuals)\n",
        "\n",
        "# Print mse_0 and mse_1\n",
        "print(\"Mean squared error with weights_0: %f\" %mse_0)\n",
        "print(\"Mean squared error with weights_1: %f\" %mse_1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean squared error with weights_0: 37.500000\n",
            "Mean squared error with weights_1: 49.890625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhWlb4zsvu6j",
        "colab_type": "text"
      },
      "source": [
        "Calculate slope and update weights\n",
        "\n",
        "Target is 6\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1isFICLF_N4x1pvs1jh04eKECBSRptS1t)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLg6ZpZZoQCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e5f0dd62-5413-4916-cc5e-c856b91d958a"
      },
      "source": [
        "# Gradient descent \n",
        "import numpy as np\n",
        "\n",
        "weights = np.array([1, 2])\n",
        "input_data = np.array([3, 4])\n",
        "target = 6\n",
        "learning_rate = 0.01\n",
        "\n",
        "preds = (weights * input_data).sum()\n",
        "error = preds - target\n",
        "print(error)\n",
        "\n",
        "gradient = 2 * input_data * error\n",
        "weights_updated = weights - learning_rate * gradient\n",
        "\n",
        "preds_updated = (weights_updated * input_data).sum()\n",
        "error_updated = preds_updated - target\n",
        "print(error_updated)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLPVYDr-5IoD",
        "colab_type": "text"
      },
      "source": [
        "Creating a Keras model  \n",
        " - specify architechture\n",
        " - compile\n",
        " - fit\n",
        " - predict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6dKKJ5T3zqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "185efe8f-6c19-4f24-9ecf-cb58cfadac9b"
      },
      "source": [
        "# Model specification\n",
        "\n",
        "!pip install tensorflow==1.14.0\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "\n",
        "data = load_iris()\n",
        "predictors = pd.DataFrame(data = data.data, columns=data.feature_names)\n",
        "target = data['target']\n",
        "\n",
        "predictors.head()\n",
        "\n",
        "n_cols = predictors.shape[1]\n",
        "print(n_cols)\n",
        "\n",
        "# Dense layers connect every layer of the previous layer to the current layer\n",
        "# we are using 100 nodes per layer, but keras will set the number actually used\n",
        "# it is not uncommon to use many many more nodes in each layer\n",
        "model = Sequential() # Weights are only connected to the next deepest layer\n",
        "model.add(Dense(100, activation='relu', input_shape=(n_cols,))) # Number of input nodes and any number of rows\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(1)) # The output layer\n",
        "\n",
        "# Adam is a method of gradient descent that adjusts the learning rate dynamically\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(predictors, target)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 83kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.27.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 44.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.1)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'get_default_graph'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-bbdfbed5be35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# we are using 100 nodes per layer, but keras will set the number actually used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# it is not uncommon to use many many more nodes in each layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Weights are only connected to the next deepest layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Number of input nodes and any number of rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_subclassed_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_subclassed_network\u001b[0;34m(self, name, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_subclassed_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_training_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_base_init\u001b[0;34m(self, name, trainable, dtype)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_uid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_uid\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0m_GRAPH_UID_DICTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_get_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         raise RuntimeError(\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;34m'It looks like you are trying to use '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;34m'a version of multi-backend Keras that '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m'does not support TensorFlow 2.0. We recommend '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: It looks like you are trying to use a version of multi-backend Keras that does not support TensorFlow 2.0. We recommend using `tf.keras`, or alternatively, downgrading to TensorFlow 1.14."
          ]
        }
      ]
    }
  ]
}