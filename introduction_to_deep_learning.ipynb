{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introduction_to_deep_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMEJYsYrMjYflcnVf4Q3Hc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TedHaley/courses/blob/master/introduction_to_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6ZF8_k3wSG1",
        "colab_type": "text"
      },
      "source": [
        "Introduction to Neural Networks\n",
        "\n",
        "Overview:  \n",
        "Models such as linear regression can be used to make predictions. It makes predictions by generating weights for some parameters and then adding those parameters together, ex:\n",
        "\n",
        "\n",
        "y = m1X1 + m2X2 + b. \n",
        "\n",
        "salary = 10,000 * years_exp + 50,000\n",
        "\n",
        "\n",
        "Where m1 and m2 are the weights. We can also have linear models that have interacting variables, such as:\n",
        "\n",
        "\n",
        "y = (m1X1 * m2X2) + b. \n",
        "\n",
        "salary = (10,000 * years_exp * 1.1* age) + 50,000\n",
        "\n",
        "\n",
        "This is a very basic example of interactivity between variables. Neural networks are similar to this but take it to the next level as they model many iteractions between many inputs, and then use those interactions as inputs for other interactions.\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/280px-Colored_neural_network.svg.png)\n",
        "\n",
        "As nodes increase, so does the ability to capture interactions.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1fdjoM2tBhsQektdjG1882m5TV9FD9rkg)\n",
        "\n",
        "Forward Propogation:  \n",
        "Forward propogation is when we place weights on the lines between nodes. The output node is equal to the sum of the input nodes times the line weights. This operation is dot product.\n",
        "\n",
        "Example:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1eJ5OLqSjIDluwHcomk2tYZbyGHVK_IWi)\n",
        "\n",
        "hidden layer node 0 = (2 * 1) + (3 * 1) = 5\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXNfK1iE6yPX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a7241430-eb2b-4fa8-d2d5-5be0986e2adf"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "input_data = np.array([2, 3])\n",
        "\n",
        "weights = {\n",
        "    'node_0': np.array([1, 1]),\n",
        "    'node_1': np.array([-1, 1]),\n",
        "    'output': np.array([2, -1]),\n",
        "}\n",
        "\n",
        "# [2, 3] * [1, 1] = (2 * 1) + (3 * 1) = 5 \n",
        "node_0_value = (input_data * weights['node_0']).sum()\n",
        "node_1_value = (input_data * weights['node_1']).sum()\n",
        "output_value = (np.array([node_0_value, node_1_value]) * weights['output']).sum()\n",
        "\n",
        "print('node 0:', node_0_value)\n",
        "print('node 1:', node_1_value)\n",
        "print('Output:', output_value)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "node 0: 5\n",
            "node 1: 1\n",
            "Output: 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MguAi-WO9fmE",
        "colab_type": "text"
      },
      "source": [
        "Activation Function:  \n",
        "Activation functions are found within the hidden layers. Activation functions allow hidden layers to capture non-linearities. One popular activation function is ReLU (rectified linear activation), with is 0 below zero, and linear above zero for x. In the below example, we are going to use tanh as our activation function.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1R_aqYRh66slQSt3YY0n1wWRTOtF8DCRa)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxRcbNhP_IYM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b3d2d8cc-5224-4926-e76b-2b8cdd64e0ed"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "input_data = np.array([2, 3])\n",
        "\n",
        "weights = {\n",
        "    'node_0': np.array([1, 1]),\n",
        "    'node_1': np.array([-1, 1]),\n",
        "    'output': np.array([2, -1]),\n",
        "}\n",
        "\n",
        "node_0_input = (input_data * weights['node_0']).sum()\n",
        "node_0_output = np.tanh(node_0_input)\n",
        "\n",
        "node_1_input = (input_data * weights['node_1']).sum()\n",
        "node_1_output = np.tanh(node_1_input)\n",
        "\n",
        "output = (np.array([node_1_input, node_1_output]) * weights['output']).sum()\n",
        "\n",
        "print('node 0:', node_0_output)\n",
        "print('node 1:', node_1_output)\n",
        "print('Output:', output)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "node 0: 0.9999092042625951\n",
            "node 1: 0.7615941559557649\n",
            "Output: 1.2384058440442351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GkkQSydRasl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bdd04b57-aaab-4211-c22a-06525c5b6b86"
      },
      "source": [
        "# Using ReLU function\n",
        "\n",
        "def relu(input):\n",
        "    '''0 below 0, x above 0.'''\n",
        "    # Calculate the value for the output of the relu function: output\n",
        "    output = max(0, input)\n",
        "    \n",
        "    # Return the value just calculated\n",
        "    return(output)\n",
        "\n",
        "# Calculate node 0 value: node_0_output\n",
        "node_0_input = (input_data * weights['node_0']).sum()\n",
        "node_0_output = relu(node_0_input)\n",
        "\n",
        "# Calculate node 1 value: node_1_output\n",
        "node_1_input = (input_data * weights['node_1']).sum()\n",
        "node_1_output = relu(node_1_input)\n",
        "\n",
        "# Put node values into array: hidden_layer_outputs\n",
        "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
        "\n",
        "# Calculate model output (do not apply relu)\n",
        "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
        "\n",
        "# Print model output\n",
        "print(model_output)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkkiP_6VTWfK",
        "colab_type": "text"
      },
      "source": [
        "Deep Networks  \n",
        " - Deep networks internally build representations of patterns in the data\n",
        " - Partially replace the need for feature engineering\n",
        " - Subsequrnt layers build increasingly sophisticated representations of raw data\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1QMVRnZHgG1FPYHgJ_c5_X1nj3Lws3ffP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voo_GJJ9WWll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fd1dc09-5215-4b44-b881-9e0bf7aa408b"
      },
      "source": [
        "# Multi-layer networks\n",
        "def predict_with_network(input_data, weights):\n",
        "    # Calculate node 0 in the first hidden layer\n",
        "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
        "    node_0_0_output = relu(node_0_0_input)\n",
        "\n",
        "    # Calculate node 1 in the first hidden layer\n",
        "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
        "    node_0_1_output = relu(node_0_1_input)\n",
        "\n",
        "    # Put node values into array: hidden_0_outputs\n",
        "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
        "    \n",
        "    # Calculate node 0 in the second hidden layer\n",
        "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n",
        "    node_1_0_output = relu(node_1_0_input)\n",
        "\n",
        "    # Calculate node 1 in the second hidden layer\n",
        "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n",
        "    node_1_1_output = relu(node_1_1_input)\n",
        "\n",
        "    # Put node values into array: hidden_1_outputs\n",
        "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
        "\n",
        "    # Calculate model output: model_output\n",
        "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
        "    \n",
        "    # Return model_output\n",
        "    return(model_output)\n",
        "\n",
        "weights = {\n",
        "    'node_0_0': np.array([2, 4]),\n",
        "    'node_0_1': np.array([ 4, -5]),\n",
        "    'node_1_0': np.array([-1,  2]),\n",
        "    'node_1_1': np.array([1, 2]),\n",
        "    'output': np.array([2, 7])\n",
        " }\n",
        "input_data = np.array([3, 5])\n",
        "output = predict_with_network(input_data, weights)\n",
        "print(output)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99GtL4umYbdb",
        "colab_type": "text"
      },
      "source": [
        "Training a Network:  \n",
        "Neural networks are trained using labelled data. The value of the weights for each line are changed to get the desired output.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1d65zM3Re55BS62mzqssqJS9XJxlvFaLJ)\n",
        "\n",
        "We use back propogation to go back and re-weight the lines to get the desired output.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=12gM57rbKz0VQ9jYzyyDuNprv0Vy7Bn-e)\n",
        "\n",
        "The challenge becomes when we are trying to make multiple accurate predictions with a static network. Each outcome is associated with its own error.\n",
        "\n",
        "Loss Function  \n",
        "The loss function is used to aggregate errors in many predictions to form a single number. This is a measure of a model's predictive performance.\n",
        "\n",
        "A common loss function is Mean Squared Error (MSE), where we square each error and take the average of the squared errors. We need to optimize the weights of the model to minimize the loss function.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1eKpqtdRmYIyBfiaC9LSpGzRofsbODznL)\n",
        "\n",
        "A simple method for optmizing this loss function is by using gradient descent. We take the derivative at the current point to find the slope of the line. We take a step in the opposite direction of the slope to go to a minima.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1V-gP00xaaRqOSzc6TR7YSsVrbMPbAjhN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDjCx4j5Ztfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}